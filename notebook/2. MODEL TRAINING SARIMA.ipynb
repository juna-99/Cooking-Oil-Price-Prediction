{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c35a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b070d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d86492",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('data/Data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the date column is in datetime format\n",
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8960bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Johor state\n",
    "Johor_data = data[data['state'] == 'Johor']\n",
    "Johor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the raw data\n",
    "print(\"Missing values in the raw data:\")\n",
    "print(Johor_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary\n",
    "Johor_summary = Johor_data.describe(include='all')\n",
    "Johor_info = Johor_data.info()\n",
    "\n",
    "# Checking for missing values\n",
    "Johor_missing_values = Johor_data.isnull().sum()\n",
    "\n",
    "Johor_summary, Johor_info, Johor_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data: average price per day\n",
    "aggregated_data = Johor_data.groupby('date')['price'].mean().reset_index()\n",
    "aggregated_data = aggregated_data.sort_values('date')\n",
    "\n",
    "# Set date as index\n",
    "aggregated_data.set_index('date', inplace=True)\n",
    "\n",
    "# Generate a full date range\n",
    "full_date_range = pd.date_range(start=aggregated_data.index.min(), end=aggregated_data.index.max(), freq='D')\n",
    "\n",
    "# Reindex the data to this full date range\n",
    "aggregated_data = aggregated_data.reindex(full_date_range)\n",
    "\n",
    "# Check for missing values before filling\n",
    "print(\"Missing values before filling:\")\n",
    "print(aggregated_data.isnull().sum())\n",
    "\n",
    "# Fill missing prices\n",
    "aggregated_data['price'] = aggregated_data['price'].ffill().bfill()\n",
    "\n",
    "# Reset index to have date as a column again\n",
    "aggregated_data.reset_index(inplace=True)\n",
    "aggregated_data.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "# Ensure there are no missing values\n",
    "assert aggregated_data.isnull().sum().sum() == 0\n",
    "\n",
    "# Check for missing values after filling\n",
    "print(\"Missing values after filling:\")\n",
    "print(aggregated_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f6452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggregated_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa5401",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to visually inspect for anomalies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aggregated_data['date'], aggregated_data['price'])\n",
    "plt.title('Price Data for Johor')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88320665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable\n",
    "Johor = aggregated_data['price']\n",
    "\n",
    "# Ensure the indices are datetime indices\n",
    "Johor.index = pd.to_datetime(aggregated_data['date'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(Johor) * 0.8)\n",
    "y_train, y_test = Johor.iloc[:train_size], Johor.iloc[train_size:]\n",
    "\n",
    "# Find the optimal SARIMA parameters using auto_arima\n",
    "stepwise_model = auto_arima(y_train, \n",
    "                            start_p=1, start_q=1,\n",
    "                            max_p=3, max_q=3, \n",
    "                            m=12,  # Seasonal period\n",
    "                            start_P=0, seasonal=True,\n",
    "                            d=1, D=1, trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "\n",
    "print(stepwise_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a738c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SARIMA model with the optimal parameters\n",
    "sarima_model = SARIMAX(y_train, \n",
    "                       order=stepwise_model.order, \n",
    "                       seasonal_order=stepwise_model.seasonal_order)\n",
    "sarima_fit = sarima_model.fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = sarima_fit.predict(start=y_train.index[0], end=y_train.index[-1], dynamic=False)\n",
    "y_pred_train = pd.Series(y_pred_train, index=y_train.index)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_test = sarima_fit.get_prediction(start=y_test.index[0], end=y_test.index[-1], dynamic=False).predicted_mean\n",
    "y_pred_test = pd.Series(y_pred_test, index=y_test.index)\n",
    "\n",
    "# Calculate error metrics for the training data\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_pred_train)\n",
    "\n",
    "# Calculate error metrics for the testing data\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_mape = mean_absolute_percentage_error(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca82c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the error metrics\n",
    "print(f'Training MAE: {train_mae}')\n",
    "print(f'Training RMSE: {train_rmse}')\n",
    "print(f'Training MAPE: {train_mape}%')\n",
    "\n",
    "print(f'Testing MAE: {test_mae}')\n",
    "print(f'Testing RMSE: {test_rmse}')\n",
    "print(f'Testing MAPE: {test_mape}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af81296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting future values\n",
    "forecast_steps = 30  # Forecast next 30 days\n",
    "forecast = sarima_fit.get_forecast(steps=forecast_steps)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "\n",
    "# Align forecast index with test data index\n",
    "forecast_index = pd.date_range(start=y_test.index[-1], periods=forecast_steps, freq='D')\n",
    "forecast_mean.index = forecast_index\n",
    "\n",
    "# Plotting the forecast\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aggregated_data['date'], aggregated_data['price'],label='Original Price', color='blue')\n",
    "plt.plot(y_train.index, y_train, label='Train', color='green')\n",
    "plt.plot(y_test.index, y_test, label='Test', color='orange')\n",
    "plt.plot(forecast_mean.index, forecast_mean, label='Forecast', color='red')\n",
    "plt.legend()\n",
    "plt.title('Price Forecast for Johor')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0448a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "residuals = sarima_fit.resid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plot_acf(residuals)\n",
    "plt.show()\n",
    "plot_pacf(residuals)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8348a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers by capping values at the 1st and 99th percentiles\n",
    "lower_bound = aggregated_data['price'].quantile(0.01)\n",
    "upper_bound = aggregated_data['price'].quantile(0.99)\n",
    "aggregated_data['price'] = np.clip(aggregated_data['price'], lower_bound, upper_bound)\n",
    "\n",
    "# Add a small constant to avoid log of zero\n",
    "aggregated_data['price'] += 1e-5\n",
    "\n",
    "# Log transform the price data\n",
    "aggregated_data['log_price'] = np.log(aggregated_data['price'])\n",
    "\n",
    "# Identify NaN values in log-transformed data\n",
    "nan_indices = aggregated_data[aggregated_data['log_price'].isna()].index\n",
    "print(\"Indices with NaN values after log transformation:\", nan_indices)\n",
    "\n",
    "# Fill NaN values using interpolation\n",
    "aggregated_data['log_price'] = aggregated_data['log_price'].interpolate(method='linear')\n",
    "\n",
    "# Ensure no NaN values remain\n",
    "print(\"Any NaN values after interpolation:\", aggregated_data['log_price'].isnull().any())\n",
    "\n",
    "# Plot the data to visually inspect for anomalies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aggregated_data['date'], aggregated_data['price'], label='Original Price')\n",
    "plt.plot(aggregated_data['date'], np.exp(aggregated_data['log_price']) - 1e-5, label='Log Transformed Price (Back Transformed)')\n",
    "plt.title('Price Data for Johor')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable\n",
    "Johor_log = aggregated_data['log_price']\n",
    "\n",
    "# Ensure the indices are datetime indices\n",
    "Johor_log.index = pd.to_datetime(aggregated_data['date'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(Johor_log) * 0.8)\n",
    "y_train_log, y_test_log = Johor_log.iloc[:train_size], Johor_log.iloc[train_size:]\n",
    "\n",
    "# Find the optimal SARIMA parameters using auto_arima\n",
    "stepwise_model_log = auto_arima(y_train_log, \n",
    "                                start_p=1, start_q=1,\n",
    "                                max_p=3, max_q=3, \n",
    "                                m=12,  # Seasonal period\n",
    "                                start_P=0, seasonal=True,\n",
    "                                d=1, D=1, trace=True,\n",
    "                                error_action='ignore',  \n",
    "                                suppress_warnings=True, \n",
    "                                stepwise=True)\n",
    "\n",
    "print(stepwise_model_log.summary())\n",
    "\n",
    "# Train the SARIMA model with the optimal parameters\n",
    "sarima_model_log = SARIMAX(y_train_log, \n",
    "                           order=stepwise_model_log.order, \n",
    "                           seasonal_order=stepwise_model_log.seasonal_order)\n",
    "sarima_fit_log = sarima_model_log.fit(disp=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train_log = sarima_fit_log.predict(start=y_train_log.index[0], end=y_train_log.index[-1], dynamic=False)\n",
    "y_pred_train_log = pd.Series(y_pred_train_log, index=y_train_log.index)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred_test_log = sarima_fit_log.get_prediction(start=y_test_log.index[0], end=y_test_log.index[-1], dynamic=False).predicted_mean\n",
    "y_pred_test_log = pd.Series(y_pred_test_log, index=y_test_log.index)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "y_pred_train = np.exp(y_pred_train_log) - 1e-5\n",
    "y_pred_test = np.exp(y_pred_test_log) - 1e-5\n",
    "\n",
    "# Calculate error metrics for the training data\n",
    "train_mae = mean_absolute_error(np.exp(y_train_log) - 1e-5, y_pred_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(np.exp(y_train_log) - 1e-5, y_pred_train))\n",
    "train_mape = mean_absolute_percentage_error(np.exp(y_train_log) - 1e-5, y_pred_train)\n",
    "\n",
    "# Calculate error metrics for the testing data\n",
    "test_mae = mean_absolute_error(np.exp(y_test_log) - 1e-5, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(np.exp(y_test_log) - 1e-5, y_pred_test))\n",
    "test_mape = mean_absolute_percentage_error(np.exp(y_test_log) - 1e-5, y_pred_test)\n",
    "\n",
    "# Print the error metrics\n",
    "print(f'Training MAE: {train_mae}')\n",
    "print(f'Training RMSE: {train_rmse}')\n",
    "print(f'Training MAPE: {train_mape}%')\n",
    "\n",
    "print(f'Testing MAE: {test_mae}')\n",
    "print(f'Testing RMSE: {test_rmse}')\n",
    "print(f'Testing MAPE: {test_mape}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting future values\n",
    "forecast_steps = 30  # Forecast next 30 days\n",
    "forecast_log = sarima_fit_log.get_forecast(steps=forecast_steps)\n",
    "forecast_mean_log = forecast_log.predicted_mean\n",
    "\n",
    "# Inverse transform the forecast\n",
    "forecast_mean = np.exp(forecast_mean_log) - 1e-5\n",
    "\n",
    "# Align forecast index with test data index\n",
    "forecast_index = pd.date_range(start=y_test_log.index[-1], periods=forecast_steps, freq='D')\n",
    "forecast_mean.index = forecast_index\n",
    "\n",
    "# Plotting the forecast\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aggregated_data['date'], aggregated_data['price'],label='Original Price', color='blue')\n",
    "plt.plot(np.exp(y_train_log).index, np.exp(y_train_log) - 1e-5, label='Train',color='green')\n",
    "plt.plot(np.exp(y_test_log).index, np.exp(y_test_log) - 1e-5, label='Test',color='orange')\n",
    "plt.plot(forecast_mean.index, forecast_mean, label='Forecast', color='red')\n",
    "plt.legend()\n",
    "plt.title('Price Forecast for Johor (Log Transformed)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ca744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "residuals_log = sarima_fit_log.resid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(residuals_log)\n",
    "plt.title('Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plot_acf(residuals_log)\n",
    "plt.show()\n",
    "plot_pacf(residuals_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40240909",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950bb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation function\n",
    "\n",
    "def rolling_forecast_cv(data, n_splits, model_order, seasonal_order):\n",
    "    \"\"\"\n",
    "    Perform rolling forecast cross-validation on time series data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: pd.Series, the time series data\n",
    "    - n_splits: int, the number of splits\n",
    "    - model_order: tuple, the (p,d,q) order of the SARIMA model\n",
    "    - seasonal_order: tuple, the (P,D,Q,m) seasonal order of the SARIMA model\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple of lists containing MAE, RMSE, and MAPE for each split\n",
    "    \"\"\"\n",
    "    mae_list = []\n",
    "    rmse_list = []\n",
    "    mape_list = []\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for train_index, test_index in tscv.split(data):\n",
    "        train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "        model = SARIMAX(train, order=model_order, seasonal_order=seasonal_order)\n",
    "        model_fit = model.fit(disp=False)\n",
    "        \n",
    "        predictions = model_fit.predict(start=test.index[0], end=test.index[-1])\n",
    "        \n",
    "        mae = mean_absolute_error(test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(test, predictions))\n",
    "        mape = np.mean(np.abs((test - predictions) / test)) * 100\n",
    "        \n",
    "        mae_list.append(mae)\n",
    "        rmse_list.append(rmse)\n",
    "        mape_list.append(mape)\n",
    "    \n",
    "    return mae_list, rmse_list, mape_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal SARIMA parameters using auto_arima\n",
    "stepwise_model_log = auto_arima(Johor_log, \n",
    "                                start_p=1, start_q=1,\n",
    "                                max_p=3, max_q=3, \n",
    "                                m=12,  # Seasonal period\n",
    "                                start_P=0, seasonal=True,\n",
    "                                d=1, D=1, trace=True,\n",
    "                                error_action='ignore',  \n",
    "                                suppress_warnings=True, \n",
    "                                stepwise=True)\n",
    "\n",
    "print(stepwise_model_log.summary())\n",
    "\n",
    "# Extract model parameters\n",
    "model_order = stepwise_model_log.order\n",
    "seasonal_order = stepwise_model_log.seasonal_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029982b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "n_splits = 5\n",
    "mae_list, rmse_list, mape_list = rolling_forecast_cv(Johor_log, n_splits, model_order, seasonal_order)\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\"MAE:\", mae_list)\n",
    "print(\"RMSE:\", rmse_list)\n",
    "print(\"MAPE:\", mape_list)\n",
    "print(\"Average MAE:\", np.mean(mae_list))\n",
    "print(\"Average RMSE:\", np.mean(rmse_list))\n",
    "print(\"Average MAPE:\", np.mean(mape_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccea45",
   "metadata": {},
   "source": [
    "### MODEL VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MAE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_splits + 1), mae_list, marker='o', label='MAE')\n",
    "plt.xlabel('Cross-Validation Split')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Cross-Validation MAE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_splits + 1), rmse_list, marker='o', label='RMSE')\n",
    "plt.xlabel('Cross-Validation Split')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.title('Cross-Validation RMSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot MAPE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, n_splits + 1), mape_list, marker='o', label='MAPE')\n",
    "plt.xlabel('Cross-Validation Split')\n",
    "plt.ylabel('Mean Absolute Percentage Error (%)')\n",
    "plt.title('Cross-Validation MAPE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c31d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted Plot for Training Data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_train_log.index, np.exp(y_train_log) - 1e-5, label='Actual')\n",
    "plt.plot(y_train_log.index, y_pred_train, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted - Training Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Actual vs Predicted Plot for Test Data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_log.index, np.exp(y_test_log) - 1e-5, label='Actual')\n",
    "plt.plot(y_test_log.index, y_pred_test, label='Predicted', alpha=0.7)\n",
    "plt.title('Actual vs Predicted - Test Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Residuals\n",
    "residuals_log = sarima_fit_log.resid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(residuals_log)\n",
    "plt.title('Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation of Residuals\n",
    "plot_acf(residuals_log)\n",
    "plt.show()\n",
    "plot_pacf(residuals_log)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save metrics to a CSV file\n",
    "metrics = {\n",
    "    'Training MAE': train_mae,\n",
    "    'Training RMSE': train_rmse,\n",
    "    'Training MAPE': train_mape,\n",
    "    'Testing MAE': test_mae,\n",
    "    'Testing RMSE': test_rmse,\n",
    "    'Testing MAPE': test_mape\n",
    "}\n",
    "\n",
    "with open('Johor_metrics.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Metric', 'Value'])\n",
    "    for key, value in metrics.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "# Save cross-validation results to a CSV file\n",
    "cv_results = {\n",
    "    'MAE': mae_list,\n",
    "    'RMSE': rmse_list,\n",
    "    'MAPE': mape_list\n",
    "}\n",
    "\n",
    "with open('Johor_cv_results.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Split', 'MAE', 'RMSE', 'MAPE'])\n",
    "    for i in range(n_splits):\n",
    "        writer.writerow([i+1, mae_list[i], rmse_list[i], mape_list[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e39231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train, test, and forecast data\n",
    "results = pd.DataFrame({\n",
    "    'date': aggregated_data['date'],\n",
    "    'original': np.exp(Johor_log) - 1e-5,\n",
    "    'train': pd.Series(np.exp(y_train_log) - 1e-5, index=y_train_log.index),\n",
    "    'test': pd.Series(np.exp(y_test_log) - 1e-5, index=y_test_log.index),\n",
    "    'forecast': pd.Series(forecast_mean, index=forecast_index)\n",
    "})\n",
    "\n",
    "# Ensure all columns align with the date index\n",
    "results.set_index('date', inplace=True)\n",
    "\n",
    "# Fill NaN values where necessary\n",
    "results['train'].fillna('', inplace=True)\n",
    "results['test'].fillna('', inplace=True)\n",
    "results['forecast'].fillna('', inplace=True)\n",
    "\n",
    "# Reset index to have date as a column again\n",
    "results.reset_index(inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "results.to_csv('JOHOR.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd88f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
